{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'itopic' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'conda install -n itopic ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as df\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import cuml\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from bertopic import BERTopic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/\"\n",
    "# file = [\"preprocess_vaccination_all_tweets\"\n",
    "#         \"preprocess_Tweets_Indonesia\",\n",
    "#         \"preprocess_Tweets_Australia\",\n",
    "#         \"preprocess_Tweets_Brazil\",\n",
    "#         \"preprocess_Tweets_Japan\",\n",
    "#         \"preprocess_Tweets_UK\",\n",
    "#         \"preprocess_Tweets_US\"]\n",
    "\n",
    "\n",
    "file = [\"Tweets_Indonesia\"]\n",
    "\n",
    "class preprocess():\n",
    "    def __init__(self):\n",
    "        return\n",
    "\n",
    "    def emoji_remove(self, tweet):\n",
    "        emoji_pattern = re.compile(\n",
    "            \"[\"\n",
    "            u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "            u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "            u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "            u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "            u\"\\U00002500-\\U00002BEF\"  # chinese char\n",
    "            u\"\\U00002702-\\U000027B0\"\n",
    "            u\"\\U00002702-\\U000027B0\"\n",
    "            u\"\\U000024C2-\\U0001F251\"\n",
    "            u\"\\U0001f926-\\U0001f937\"\n",
    "            u\"\\U00010000-\\U0010ffff\"\n",
    "            u\"\\u2640-\\u2642\"\n",
    "            u\"\\u2600-\\u2B55\"\n",
    "            u\"\\u200d\"\n",
    "            u\"\\u23cf\"\n",
    "            u\"\\u23e9\"\n",
    "            u\"\\u231a\"\n",
    "            u\"\\ufe0f\"  # dingbats\n",
    "            u\"\\u3030\"\n",
    "              \"]+\", re.UNICODE)\n",
    "        return emoji_pattern.sub(r'', tweet)\n",
    "\n",
    "    def rt_remove(self, tweet):\n",
    "        for word in tweet.split():\n",
    "            if word[0] == '@':\n",
    "                tweet = tweet.replace(word, \"\")\n",
    "            if word == \"RT\":\n",
    "                tweet = tweet.replace(word, \"\")\n",
    "        return tweet\n",
    "\n",
    "    def hashtag_remove(self, tweet):\n",
    "        return re.sub(r'\\#w+', '', tweet)\n",
    "\n",
    "    def preprocess_tweet(self, tweets):\n",
    "        tweets = self.rt_remove(tweets)\n",
    "        tweets = self.emoji_remove(tweets)\n",
    "        tweets = self.hashtag_remove(tweets)\n",
    "        # print(tweets)\n",
    "        return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for filename in file:\n",
    "    print(\"\\nProcessing: \" + filename + '\\n')\n",
    "    ddf = df.read_csv(data_path + filename + '.csv',\n",
    "                    usecols=[\"created_at\", \"text\"],\n",
    "                    sep = ',',\n",
    "                    on_bad_lines='skip')\n",
    "    ddf['text'] = ddf['text'].astype(str)\n",
    "    ddf['text'].fillna(\"\")\n",
    "    preprocess_class = preprocess()\n",
    "\n",
    "    ddf['text']=ddf.text.apply(lambda x :preprocess_class.preprocess_tweet(x))\n",
    "    # print(ddf.text.head())\n",
    "    print(\"Running sentence transformer\")\n",
    "    model = SentenceTransformer('distilbert-base-nli-mean-tokens',\n",
    "                                device='cuda')\n",
    "    embeddings = model.encode(ddf['text'], show_progress_bar=True)\n",
    "\n",
    "    print(\"Running UMAP\")\n",
    "    umap = cuml.manifold.UMAP(n_components=10, n_neighbors=15,\n",
    "                            min_dist=0.0, random_state=12)\n",
    "    reduced_data = umap.fit_transform(embeddings)\n",
    "\n",
    "    print(\"Running HDBSCAN\")\n",
    "    clusterer = cuml.cluster.hdbscan.HDBSCAN(min_samples = 10,\n",
    "                                            gen_min_span_tree=True)\n",
    "    clusterer.fit(reduced_data)\n",
    "    topic_model = BERTopic(umap_model=umap, hdbscan_model=clusterer)\n",
    "    topics, probs = topic_model.fit_transform(ddf[\"text\"])\n",
    "    # soft_clusters = cuml.cluster.hdbscan.all_points_membership_vectors(clusterer)\n",
    "    topic_model.get_topic_freq().head()\n",
    "    topic_model.visualize_topics()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rapids-23.02",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
